{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of contents\n",
        "1. [Introduction](https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/Introduction.ipynb)(This notebook)\n",
        "    1. [Quantization](#quanti)\n",
        "    2. [Distillation](#distill)\n",
        "    3. [Pruning](#prune)\n",
        "    4. [Low-Rank Approximation](#la)\n",
        "\n",
        "<p align=\"right\">\n",
        "  <img src=\"https://github.com/datacraft-paris/2311-Cerisara-LLM/blob/main/illustrations/small_llama.png?raw=true:, width=500\" alt=\"small llama\" width=800 class=\"right\">\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "T8o2BTuX2QHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "LLMs (Large Language Models) are highly useful for various tasks but are too large and quite expensive to (re-)train, fine-tune, and deploy.\n",
        "\n",
        "There are several methods for compressing a model:\n",
        "\n",
        "# **Quantization** <a name=\"quanti\"></a>\n",
        "<p align=\"right\">\n",
        "  <img src=\"https://github.com/datacraft-paris/2311-Cerisara-LLM/blob/main/illustrations/quanti.png?raw=true:, width=500\" alt=\"quantization\" width=800 class=\"right\">\n",
        "</p>\n",
        "\n",
        "The image illustrate a naive uniform quantization.\n",
        "\n",
        "*image source: https://blog.gopenai.com/the-llm-revolution-boosting-computing-capacity-with-quantization-methods-b8666cdb4b6a*\n",
        "\n",
        "***Pros*** üëçüèΩ\\\n",
        "This method rounds the model weights to a smaller precision, making the models lightweight with minimal loss of performance. It is relatively easy to implement and yields good practical results.\n",
        "\n",
        "***Cons*** üëéüèΩ\\\n",
        "The quantized model cannot be trained or fine-tuned; it is only used for inference.\n",
        "\n",
        "*Example of projects*\n",
        "- [llama.cpp](https://github.com/ggerganov/llama.cpp)\n",
        "- [Ollama](https://ollama.com/): uses llama.cpp in background\n",
        "- [GPTQ](https://github.com/IST-DASLab/gptq): One-shot quantization from weights. Efficient on GPU.\n",
        "- [AWQ](https://github.com/mit-han-lab/llm-awq): Few-shot quantization using activations. Efficient on GPU and works well for multimodal and instruction tuned LLM.\n",
        "\n",
        "# **Distillation** <a name=\"distill\"></a>\n",
        "<p align=\"right\">\n",
        "  <img src=\"https://github.com/datacraft-paris/2311-Cerisara-LLM/blob/main/illustrations/distill.png?raw=true:, width=500\" alt=\"quantization\" width=800 class=\"right\">\n",
        "</p>\n",
        "This method trains a small model (student) to match the performance of the large base model (teacher).\n",
        "\n",
        "***Pros*** üëçüèΩ\n",
        "- Often produces high-quality models that match the performance of the base model.\n",
        "- The resulting small model can be used for fine-tuning.\n",
        "\n",
        "***Cons*** üëéüèΩ\\\n",
        "Requires a large amount of training data as the student's parameters are randomly initialized.\n",
        "\n",
        "# **Pruning** <a name=\"prune\"></a>\n",
        "<p align=\"right\">\n",
        "  <img src=\"https://github.com/datacraft-paris/2311-Cerisara-LLM/blob/main/illustrations/pruning.png?raw=true:, width=500\" alt=\"quantization\" width=800 class=\"right\">\n",
        "</p>\n",
        "Pruning involves setting certain neuron connections, neurons, or even entire layers to zero. This reduces the model's weight as the values become zero, and in some cases, parameters can be removed.\n",
        "\n",
        "***Pros*** üëçüèΩ\n",
        "- Depending on the methode, it can be easy to implement.\n",
        "- Numerous libraries support pruning, and some methods are integrated natively into PyTorch.\n",
        "\n",
        "***Cons*** üëéüèΩ\\\n",
        "The resulting model is often \"destructured\" compared to the base model. For example, in a transformers model, the resulting model may have a different number of attention heads across layers, deviating from a standard architecture. This has consequences for the efficient use of GPUs.\n",
        "\n",
        "*Example of projects*\n",
        "- [LLM-Pruner](https://github.com/horseee/LLM-Pruner)\n",
        "- [LLM-Shearing](https://github.com/princeton-nlp/LLM-Shearing)\n",
        "\n",
        "# **Low-rank Approximation** <a name=\"la\"></a>\n",
        "This method involves approximating the matrices of linear layers with smaller-sized matrices.\n",
        "\n",
        "***Pros*** üëçüèΩ\n",
        "- Easy and cost-effective to implement.\n",
        "- Does not disrupt the resulting model's structure.\n",
        "- Does not fundamentally change the architecture of the resulting model.\n",
        "- The resulting model can be retrained or fine-tuned.\n",
        "\n",
        "***Cons*** üëéüèΩ\\\n",
        "Introduces a significant loss of performance."
      ],
      "metadata": {
        "id": "d2QNAuKGAf2J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PKBqLGXgKaD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
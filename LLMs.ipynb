{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPxFa6GJxTDkQkSQ9r7XzW3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of contents\n",
        "1. [Introduction](https://www.google.com/)\n",
        "2. [A Brief Overview of LLMs](#paragraph1) (This notebook)\n",
        "    1. [Background: _Decoder-only_ Language Model](#language_models_are_conditional_probabilities\")\n",
        "    2. [Transformers](#transformers)\n",
        "    3. [Large Language Model](#llms)\n",
        "    4. [Exercices](#exercices)\n"
      ],
      "metadata": {
        "id": "lzFvrBylLcQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background: _Decoder-only_ Language Model (LM) <a name=\"language_models_are_conditional_probabilities\"></a>"
      ],
      "metadata": {
        "id": "3bvX3IlycUWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Traditionally, a language model (LM) is just a function that assigns a probability to a given sequence of words (it can also be letters, tokens, etc). For a sequence of words $\\textbf{s}$ and a trained language model $P_{LM}$, the language model can assign a probability to this sequence:\n",
        "\n",
        "> $$P(\\textbf{s}) = \\prod_{i=1}^{|\\textbf{s}|} P_{LM}({s_{i}}|s_{<i})$$\n",
        "where $s_{<i}$ stands for all the previous words.\n",
        "\n",
        "for example, for the sentence \"He runs fast\":\n",
        "\n",
        "> $P$(He runs fast) = $P_{LM}$(\"He\") $\\times$ $P_{LM}$(\"runs\" | \"He\") $\\times$ $P_{LM}$(\"fast\" | \"he runs\")\n",
        "\n",
        "\n",
        "Indeed, we can see this as **the probability of predicting the next word** at each timestamp in the sentence.\n",
        "\n",
        "In the case of neural language model, words are represented as vectors. We can illustrate the language modelling task as:\n",
        "\n",
        "<p align=\"right\">\n",
        "  <img src=\"https://github.com/datacraft-paris/2311-Cerisara-LLM/blob/main/illustrations/lm_head.png?raw=true:, width=400\" alt=\"transformer\" width=800 class=\"right\">\n",
        "</p>\n",
        "\n",
        "Each word vector in the sentence goes through the Linear layer that predicts the score for the possible following words. We normalize then these scores in order to get probabilities.\n",
        "\n",
        "As you can see in the figure, the model don't always give higher score to the right tokens. The learning procedure consists of ensuring that the model assigns a higher probability to the next true tokens.\n",
        "\n",
        "Also, as you can see in the figure, if we wan to predict right tokens, we need to find a way of getting meaningfull word vectors. One model than can give us meaningfull word vectors is the Transformer model.\n",
        "\n"
      ],
      "metadata": {
        "id": "HPRgVLWtJuL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers <a name=\"transformers\"></a>"
      ],
      "metadata": {
        "id": "PKmYb1FwMIDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformers architecture was first developed for the machine translation task. It was designed to model the long-range dependenies between words in a sentence, which is essential for machine translation task.\n",
        "\n",
        "The transformer has two core components: the Attention module and the MLP module. We will not define the attention mechanism in details here, nor the transformer architecture itself. What interest us is the parameters prensent in the Transformer.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/datacraft-paris/2311-Cerisara-LLM/blob/main/illustrations/transformer_parameters.png?raw=true:, width=300\" alt=\"transformer\" width=600 class=\"center\">\n",
        "</p>\n",
        "\n",
        "\n",
        "The transformer takes as input a sequence of vectors $E$, and also outputs a sequence of vectors $C$ of the same length. So what is the difference between the two sequences ?\n",
        "\n",
        "The difference is that:\n",
        "1. $E$, the first sequence of vectors, consists of vectors that are independent of each other.\n",
        "2. $C$, the vectors outputed by the Transformer, are contextualized, meaning each vector incorporates information about all other tokens in the sequence.\n",
        "\n",
        "So, the Transformer litterally _transforms_ the input sequence, and this transformation is a contextualization. These contextual vectors are then passed to the LM Head.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JLntRrzlpoFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM = LM trained with *HUGE* Transformers on *HUGE* text corpora. <a name=\"llms\"></a>\n",
        "\n",
        "The transformer takes as inputs a $X$ sequence of vectors and output $C$ also a sequence of vectors. As such, this is not a language model, but just a neural network that can be used on any task / domain (computer vision, speech processing, etc). To perform language modeling, we add a matrix $W \\in \\mathbb{R}^{d * |V|}$ on top of the transformer's output. This matrix contains one vector for each word (or token) seen in the training set. Given an output vector $\\textbf{c}$ of the transformer, the probability of a given word $v$ in the vocabulary $V$ is computed as:\n",
        "\n",
        "$$\n",
        "P_{LM}(y = v|\\textbf{c}) = \\frac{W_{v} \\cdot c}{\\sum\\limits_{v' \\in V}W_{v'} \\cdot c}\n",
        "$$\n",
        "\n",
        "For example, let's say we have the input sentence \"He runs fast\" that got forward into the transformer, giving the output $C=[\\textbf{c}_{\\small He}, \\textbf{c}_{\\small runs}, \\textbf{c}_{\\small fast}]$. The probability that \"fast\" follows the word \"runs\" is computed as:\n",
        "\n",
        "$$\n",
        "P_{LM}(fast|\\textbf{c}_{\\small runs}; W) = \\frac{W_{fast} \\cdot \\textbf{c}_{\\small runs}}{\\sum\\limits_{v' \\in V}W_{v'} \\cdot \\textbf{c}_{\\small runs}}\n",
        "$$\n",
        "\n",
        "This is the language modelling objective applied to a tranformer. Usually, the transformer language model is trained on a huge quantity of text corpora $D$ chunked into long sequences $s$ (for example Llama is trained on sequences of 4096 tokens).\n",
        "\n",
        "Training the transformer language model is achieved using the maximum likelihood of training corpora $D$:\n",
        "\n",
        ">$$\n",
        "\\underset{\\theta}{\\mathrm{argmax}} \\prod_{s \\in D} \\prod_{i=1}^{|\\textbf{s}|} P_{LM}({s_{i}}|s_{<i}; \\theta)\n",
        "$$\n",
        "where $\\theta$ is the parameter of the LLM: the matrix $W$ and all the trainable matrices in the transformer model.\n",
        "\n",
        "When the training corpora is enough huge (>1T tokens) and the transformer contains enough parameters (>7B parameters), the language model learns to perform well on different language tasks (translation, classification, summarization, coding, etc.)"
      ],
      "metadata": {
        "id": "hfMqwvpNMMdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercices <a name=\"exercices\"></a>"
      ],
      "metadata": {
        "id": "xBM8zLV0kEiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "V5gEQNG0Gale"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dissecting an LLM"
      ],
      "metadata": {
        "id": "e0CBek9xEQ3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download the StableLM 3B and print the model."
      ],
      "metadata": {
        "id": "nuwokSvFEYkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gtcsCnRTEXJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Identify the parameters of the attention module. What are their dimensions ?"
      ],
      "metadata": {
        "id": "EajLppv8E-nT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_3LdEHQFBbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Identify the paramaters of the MLP module. What are their dimensions ?"
      ],
      "metadata": {
        "id": "RhBZhsTiKbee"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_3UeHTYnKzk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Identify the language modelling matrix (LM Head), what is its dimension ?"
      ],
      "metadata": {
        "id": "43n52nlDKo7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lW1odwz7Kz7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Get the tokenizer associated with the LLM. Tokenize the sentence \"He runs\" and forward it into the LLM in order to get the last hidden states. What is the dimension ?"
      ],
      "metadata": {
        "id": "SU4jb8dKkKKb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBS8oElOM5q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The numer of parameters of the LLM"
      ],
      "metadata": {
        "id": "F3atq4nmBtuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is exactly the number of paramaters of the model ?"
      ],
      "metadata": {
        "id": "uYfqWWNHTjo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the methode .num_parameters() of the model"
      ],
      "metadata": {
        "id": "te3M09TLX9yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How many parameters does the attention module have ? and for MLP module ? In terms of percentage ?"
      ],
      "metadata": {
        "id": "3FgT3kIJUo0E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWdtStoGBvsK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM08jWmM1Aic0vPJCYekQOv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2LilP0B9TFJ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from pandas import DataFrame\n",
        "from transformers import pipeline, AutoModelForCausalLM, PreTrainedModel, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Load the model\n",
        "llm = AutoModelForCausalLM.from_pretrained(\"stabilityai/stablelm-3b-4e1t\",\n",
        "                                           torch_dtype=torch.bfloat16)\n",
        "llm = llm.cuda()\n",
        "lr_llm = copy.deepcopy(llm)"
      ],
      "metadata": {
        "id": "GGKL68Sn-PDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LowRankLinear(torch.nn.Module):\n",
        "    def __init__(self, in_features, rank, out_features):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Sequential(\n",
        "            torch.nn.Linear(in_features=in_features,\n",
        "                            out_features=rank,\n",
        "                            bias=False,\n",
        "                            dtype=torch.bfloat16),\n",
        "            torch.nn.Linear(in_features=rank,\n",
        "                            out_features=out_features,\n",
        "                            bias=False,\n",
        "                            dtype=torch.bfloat16)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "BhYloJ9r9dus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setmodule(module, target_module, value):\n",
        "    \"\"\"Set a target module from in a given module.\"\"\"\n",
        "    submodules = target_module.split(\".\", 1)\n",
        "    if len(submodules) == 1:\n",
        "        setattr(module, submodules[0], value)\n",
        "    else:\n",
        "        setmodule(getattr(module, submodules[0]), submodules[-1], value)"
      ],
      "metadata": {
        "id": "8HxL2L6m9i8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lowrank_weights(path, llm):\n",
        "    \"\"\"\n",
        "    Loads distilled Low-Rank Linears into the LLM.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    - path: folder containing the saved distilled Low-Rank Linears.\n",
        "    - llm: LLM on which to load the Low-Rank Linear.\n",
        "    \"\"\"\n",
        "    total = sum(1 for _ in Path(path).glob(\"*.pt\"))\n",
        "    for weights in tqdm(Path(path).glob(\"*.pt\"), total=total):\n",
        "        loaded_weights = torch.load(weights, map_location=torch.device('cpu'))\n",
        "        in_features = max(loaded_weights[\"linear.0.weight\"].shape)\n",
        "        rank = min(loaded_weights[\"linear.0.weight\"].shape)\n",
        "        out_features = max(loaded_weights[\"linear.1.weight\"].shape)\n",
        "        lowrank_linear = LowRankLinear(in_features, rank, out_features)\n",
        "        lowrank_linear.load_state_dict(loaded_weights)\n",
        "        setmodule(llm, Path(weights).stem, lowrank_linear)"
      ],
      "metadata": {
        "id": "r8vx9Gvi9lhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_lowrak_weigh(\"TODO\")"
      ],
      "metadata": {
        "id": "DpS4cbUu9nqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hand Test"
      ],
      "metadata": {
        "id": "6J3VCrFP9txi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
        "                                          trust_remote_code=True)\n",
        "pipe = pipeline(\"text-generation\", model=lr_llm, tokenizer=tokenizer, do_sample=False)"
      ],
      "metadata": {
        "id": "Hl82Uv8-9sVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipe(\"One day, I will\", max_new_tokens=32, min_new_tokens=8)[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "uXsbNtJ59z_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perplexity"
      ],
      "metadata": {
        "id": "OVKHRYqb-i_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to assess the quality of the Low-Rank LLM on a given test set. We can achieve this by using the Perplexity metric, which measure the accuracy of the model at predicting the Test set.\n",
        "\n",
        "The cross-entropy loss of the next token defines the log-probability of the gold-next token at each timestamps in the sequence:\n",
        "\n",
        ">$$\n",
        "CE(x_{i+1}) = log\\;p(y=x_{i+1}|x_{1}, x_{2}, ..., x_{i})\n",
        "$$\n",
        "Where $x_{i+1}$ means the next token following the token $x_{i}$\n",
        "\n",
        "Then, the perplexity is just defined as:\n",
        "\n",
        "$$\n",
        "perplexity = exp(CE)\n",
        "$$\n",
        "\n",
        "Use the test corpus and compute the perplexity for the low-rank LLM and the base LLM."
      ],
      "metadata": {
        "id": "45ornlT_zQpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = \"\""
      ],
      "metadata": {
        "id": "ojEfw6XR_S4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def forward_dataset(dataset: Dataset,\n",
        "                    llm: PreTrainedModel,\n",
        "                    batch_size: int=64\n",
        "                    ) -> dict:\n",
        "    \"\"\"Forwards all the dataset through the LLM and computes the perplexity.\"\"\"\n",
        "    dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "        inputs = batch[\"input_ids\"].to(llm.device)\n",
        "        loss = llm(input_ids=inputs, labels=inputs)\n",
        "        yield {\"perplexity\": \"TODO: compute the perplexity\"}"
      ],
      "metadata": {
        "id": "-7bR-hUR-pBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNKf2U4Q36Z-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
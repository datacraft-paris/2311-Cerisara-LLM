{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPRVMBhby3zKZKRu+ny1il6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Introduction](https://www.google.com/)\n",
        "2. [A Brief Overview of LLMs](https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/LLMs.ipynb)\n",
        "3. Preparing the data (This notebook)\n",
        "    1. [Setup](#setup)\n",
        "    2. [Downloading the data](#download)\n",
        "    3. [Preprocess the data](#preprocess)\n",
        "      1. [Tokenization](#tokenize)\n",
        "      2. [Group the texts](#group)\n",
        "    4. [Run the preparation](#run)"
      ],
      "metadata": {
        "id": "ksYtjzOy79Pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup <a name=\"setup\"></a>"
      ],
      "metadata": {
        "id": "K5kr7zKMvJH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets # we will use huggingface datasets, easy to use and the hub contains many big text corpora for training LLMs."
      ],
      "metadata": {
        "id": "PWdJ94ydvLYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Iterator, Dict, List, Tuple\n",
        "from itertools import chain\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, load_from_disk, concatenate_datasets, Features, Dataset, DatasetDict, features\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "p5uboKDiyOUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the data <a name=\"download\"></a>"
      ],
      "metadata": {
        "id": "KHhfswrm-WJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We manipulate huge corpora (>1 trillion tokens) when we work on LLMs. However, for this experiments, we only need ~100 millions of tokens.\n",
        "\n",
        "We will use RedPajama, a data for training LLMs, which contains more than 1 trillion tokens. This data is huge and we can't manipulate it. We only need a subset of this corpus.\n",
        "\n",
        "The following cells extract a subset of RedPajama in a efficient way."
      ],
      "metadata": {
        "id": "Qs8Ty2oGmwoX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu3F57AJ71i5"
      },
      "outputs": [],
      "source": [
        "# Streaming the data so we don't need to download all the data (1T tokens)\n",
        "def stream_data(corpus: str) -> Iterator[str]:\n",
        "    \"\"\"Streams the huggingface dataset.\"\"\"\n",
        "    dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T\",\n",
        "                            corpus,\n",
        "                            streaming=True)\n",
        "    for item in dataset[\"train\"]:\n",
        "        metadata = eval(item[\"meta\"])\n",
        "        if \"language\" not in metadata:\n",
        "            raise ValueError(f\"The data '{corpus}' doesn't contain any information about languages.\")\n",
        "        if corpus != \"github\" and metadata[\"language\"] != \"en\": # only english texts\n",
        "            continue\n",
        "        yield item[\"text\"]\n",
        "\n",
        "# only get a subset of items.\n",
        "def subset_dataset(subset: int,\n",
        "                   corpus: str\n",
        "                   ) -> Iterator[Dict[str, str]]:\n",
        "    \"\"\"Extract only a subset of the whole dataset.\"\"\"\n",
        "    for idx, item in tqdm(enumerate(stream_data(corpus), 1),\n",
        "                          total=subset,\n",
        "                          desc=corpus):\n",
        "        yield {\n",
        "            \"text\": item,\n",
        "            \"corpus\": corpus\n",
        "        }\n",
        "        if idx == subset:\n",
        "            break\n",
        "\n",
        "# Create the huggingface dataset\n",
        "def dataset_from_generator(subset_mapping: dict) -> None:\n",
        "    \"\"\"Creates hf datasets object.\"\"\"\n",
        "    data_features = Features({\n",
        "        \"text\": features.Value(\"string\"),\n",
        "        \"corpus\": features.Value(\"string\")\n",
        "    })\n",
        "\n",
        "    dataset = DatasetDict({corpus: Dataset.from_generator(\n",
        "                                        subset_dataset,\n",
        "                                        features=data_features,\n",
        "                                        gen_kwargs={\"subset\": subset_mapping[corpus], \"corpus\": corpus}\n",
        "                                        )\n",
        "                                    for corpus in subset_mapping})\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we use different subset for each category because some of them have longer tokens.\n",
        "# This means for the same number of documents, they don't have the same number tokens.\n",
        "# For example, 'arxiv' tends to have longer documents campared to 'c4' or 'stackexchange'\n",
        "dataset = dataset_from_generator({\"c4\": 256_000,\n",
        "                                  \"arxiv\": 16_000,\n",
        "                                  \"stackexchange\": 256_000,\n",
        "                                  \"github\": 48_000})"
      ],
      "metadata": {
        "id": "5OtWgdNBvqdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the data <a name=\"preprocess\"></a>"
      ],
      "metadata": {
        "id": "OCq9Ci2vx6lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization <a name=\"tokenize\"></a>"
      ],
      "metadata": {
        "id": "_XP7F973yZo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer are one of the most important part of the LLM, because it's about giving to the LLM meaningfull piece of words.\n",
        "\n",
        "Tokenizing big corpora can be challenging as it can be quite slow. We will use multiprocessing so we can parallelize the process."
      ],
      "metadata": {
        "id": "Yz-s3DOTo-j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of CPUs:\", os.cpu_count())"
      ],
      "metadata": {
        "id": "VDPe7AjPpwg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(tokenizer,\n",
        "                     dataset: Dataset,\n",
        "                     target_colum: str=\"text\",\n",
        "                     return_attention_mask: bool=False,\n",
        "                     batched: bool=True,\n",
        "                     batch_size: int=64\n",
        "                     ) -> Dataset:\n",
        "    \"\"\"Tokenize the dataset.\"\"\"\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda examples: tokenizer(examples[target_colum],\n",
        "                                   return_attention_mask=return_attention_mask),\n",
        "        num_proc=os.cpu_count(),\n",
        "        batched=batched,\n",
        "        batch_size=batch_size,\n",
        "        desc=\"Running tokenizer on dataset\",\n",
        "    )\n",
        "    return tokenized_dataset"
      ],
      "metadata": {
        "id": "3WCp1A-NvwcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group the texts <a name=\"group\"></a>"
      ],
      "metadata": {
        "id": "PiAv9ALMyc0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batching the forward into the LLM is necessary for efficient use of the GPU. The goal is to fill maximimally the memory of the GPU, with tokens only (without padding tokens). A common way to do that is to group the text into a chunks of _n_ tokens."
      ],
      "metadata": {
        "id": "-myqZ0aap70V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(dataset: Dataset,\n",
        "                max_length: int=512,\n",
        "                batch_size: int=128,\n",
        "                return_labels: bool=False\n",
        "                ) -> Dataset:\n",
        "    \"\"\"Grouping texts to max_length.\"\"\"\n",
        "    def group(examples):\n",
        "        # Concatenate all texts.\n",
        "        concatenated_examples = {k: list(chain(*examples[k])) if k != \"corpus\" else examples[k]\n",
        "                                 for k in examples.keys()}\n",
        "        total_length = len(concatenated_examples[\"input_ids\"])\n",
        "        total_length = (total_length // max_length) * max_length\n",
        "        # Split by chunks of max_len.\n",
        "        result = {\n",
        "            k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "        if return_labels:\n",
        "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "        return result\n",
        "    dataset = dataset.map(\n",
        "        group,\n",
        "        batched=True,\n",
        "        batch_size=batch_size,\n",
        "        num_proc=os.cpu_count(),\n",
        "        desc=f\"Grouping texts in chunks of {max_length}\",\n",
        "        )\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "2Aoim-JzyekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the preparation <a name=\"run\"></a>"
      ],
      "metadata": {
        "id": "5lxsJ9WbymA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(dataset: Dataset,\n",
        "                 subset: int=48_000,\n",
        "                 max_length: int=1024,\n",
        "                 ) -> Dataset:\n",
        "    \"\"\"Sample the same number of tokens for each corpus and split in train/test.\"\"\"\n",
        "    for corpus in dataset:\n",
        "        dataset[corpus] = dataset[corpus].remove_columns(\"corpus\").add_column(\"corpus\", [corpus] * len(dataset[corpus]))\n",
        "        if subset is None or len(dataset[corpus]) <= subset:\n",
        "            continue\n",
        "        dataset[corpus] = dataset[corpus].select(range(subset))\n",
        "    dataset = dataset.shuffle()\n",
        "    train = {}\n",
        "    test = {}\n",
        "    for corpus in dataset:\n",
        "        train_test = dataset[corpus].train_test_split(0.2)\n",
        "        train[corpus] = train_test[\"train\"]\n",
        "        test[corpus] = train_test[\"test\"]\n",
        "    train = DatasetDict(train)\n",
        "    test = DatasetDict(test)\n",
        "    train = concatenate_datasets(train.values())\n",
        "    test = concatenate_datasets(test.values())\n",
        "    train_tokens = (len(train) * max_length)\n",
        "    test_tokens = (len(test) * max_length)\n",
        "    print(f\"Number of training tokens: {train_tokens:,}. Number of testing tokens: {test_tokens:,}\")\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "lSoayIAdyrme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have defined all the required methods, we can the data preparation.\n",
        "\n",
        "In your opinion, why do we group the sequences?"
      ],
      "metadata": {
        "id": "zcfoCtGhkvyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n",
        "tokenized_dataset = tokenize_dataset(tokenizer=tokenizer,\n",
        "                                     dataset=dataset,\n",
        "                                     batch_size=2)\n",
        "grouped_dataset = group_texts(dataset=tokenized_dataset, batch_size=2)\n",
        "train, test = prepare_data(dataset=grouped_dataset)\n",
        "# TODO: save the train and test dataset in two different folders."
      ],
      "metadata": {
        "id": "2udRzw3hjOOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
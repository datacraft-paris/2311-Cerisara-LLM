{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP3Stk5AgSjQeEqKHPUbmcI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Introduction](https://www.google.com/)\n",
        "2. [A Brief Overview of LLMs](https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/LLMs.ipynb)\n",
        "    1. [Background: _Decoder-only_ Language Model](#language_models_are_conditional_probabilities\")\n",
        "    2. [Transformers](#transformers)\n",
        "    3. [Large Language Model](#llms)\n",
        "    4. [Exercices](#exercices)\n",
        "3. Preparing the data (This notebook)\n",
        "    1. [Setup](#setup)\n",
        "    2. [Downloading the data](#download)\n",
        "    3. [Preprocess the data](#preprocess)\n",
        "      1. [Tokenization](#tokenize)\n",
        "      2. [Group the texts](#group)\n",
        "    4. [Run the preparation](#run)"
      ],
      "metadata": {
        "id": "ksYtjzOy79Pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup <a name=\"setup\"></a>"
      ],
      "metadata": {
        "id": "K5kr7zKMvJH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "PWdJ94ydvLYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Iterator, Dict, List, Tuple\n",
        "from itertools import chain\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset, load_from_disk, concatenate_datasets, Features, Dataset, DatasetDict, features"
      ],
      "metadata": {
        "id": "p5uboKDiyOUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the data <a name=\"download\"></a>"
      ],
      "metadata": {
        "id": "KHhfswrm-WJy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu3F57AJ71i5"
      },
      "outputs": [],
      "source": [
        "def stream_data(corpus: str) -> Iterator[str]:\n",
        "    \"\"\"Streams the huggingface dataset.\"\"\"\n",
        "    dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T\",\n",
        "                            corpus,\n",
        "                            streaming=True)\n",
        "    for item in dataset[\"train\"]:\n",
        "        metadata = eval(item[\"meta\"])\n",
        "        if \"language\" not in metadata:\n",
        "            raise ValueError(f\"The data '{corpus}' does'nt contain any information about languages.\")\n",
        "        if corpus != \"github\" and metadata[\"language\"] != \"en\": # only took english\n",
        "            continue\n",
        "        yield item[\"text\"]\n",
        "\n",
        "def subset_dataset(subset: int,\n",
        "                   corpus: str\n",
        "                   ) -> Iterator[Dict[str, str]]:\n",
        "    \"\"\"Extract only a subset of the whole dataset.\"\"\"\n",
        "    for idx, item in tqdm(enumerate(stream_data(corpus), 1),\n",
        "                          total=subset,\n",
        "                          desc=corpus):\n",
        "        yield {\n",
        "            \"text\": item,\n",
        "            \"corpus\": corpus\n",
        "        }\n",
        "        if idx == subset:\n",
        "            break\n",
        "\n",
        "def dataset_from_generator(output_folder: str, subset: int=32_000) -> None:\n",
        "    \"\"\"Creates hf datasets object and saves it to the disk.\"\"\"\n",
        "    output_folder = Path(output_folder)\n",
        "    corpora = {\"c4\", \"arxiv\", \"stackexchange\", \"github\"}\n",
        "    output_folder.mkdir(exist_ok=True, parents=True)\n",
        "    data_features = Features({\n",
        "        \"text\": features.Value(\"string\"),\n",
        "        \"corpus\": features.Value(\"string\")\n",
        "    })\n",
        "    dataset = DatasetDict({corpus: Dataset.from_generator(\n",
        "                                        subset_dataset,\n",
        "                                        features=data_features,\n",
        "                                        gen_kwargs={\"subset\": subset, \"corpus\": corpus}\n",
        "                                        )\n",
        "                                    for corpus in corpora})\n",
        "\n",
        "    dataset.save_to_disk(output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_from_generator(\"data\")"
      ],
      "metadata": {
        "id": "5OtWgdNBvqdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the data <a name=\"preprocess\"></a>"
      ],
      "metadata": {
        "id": "OCq9Ci2vx6lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization <a name=\"tokenize\"></a>"
      ],
      "metadata": {
        "id": "_XP7F973yZo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(tokenizer,\n",
        "                     dataset: Dataset,\n",
        "                     remove_columns: List[str],\n",
        "                     target_colum: str=\"text\",\n",
        "                     return_attention_mask: bool=False,\n",
        "                     batched: bool=True,\n",
        "                     batch_size: int=64\n",
        "                     ) -> Dataset:\n",
        "    \"\"\"Tokenize the dataset.\"\"\"\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda examples: tokenizer(examples[target_colum],\n",
        "                                   return_attention_mask=return_attention_mask),\n",
        "        num_proc=os.cpu_count(),\n",
        "        batched=batched,\n",
        "        batch_size=batch_size,\n",
        "        remove_columns=remove_columns,\n",
        "        desc=\"Running tokenizer on dataset\",\n",
        "    )\n",
        "    return tokenized_dataset"
      ],
      "metadata": {
        "id": "3WCp1A-NvwcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group the texts <a name=\"group\"></a>"
      ],
      "metadata": {
        "id": "PiAv9ALMyc0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(dataset: Dataset,\n",
        "                max_length: int=1024,\n",
        "                batch_size: int=128,\n",
        "                return_labels: bool=False\n",
        "                ) -> Dataset:\n",
        "    \"\"\"Grouping texts to max_length.\"\"\"\n",
        "    def group(examples):\n",
        "        # Concatenate all texts.\n",
        "        concatenated_examples = {k: list(chain(*examples[k])) if k != \"corpus\" else examples[k]\n",
        "                                 for k in examples.keys()}\n",
        "        total_length = len(concatenated_examples[\"input_ids\"])\n",
        "        total_length = (total_length // max_length) * max_length\n",
        "        # Split by chunks of max_len.\n",
        "        result = {\n",
        "            k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "        if return_labels:\n",
        "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "        return result\n",
        "    dataset = dataset.map(\n",
        "        group,\n",
        "        batched=True,\n",
        "        batch_size=batch_size,\n",
        "        num_proc=os.cpu_count(),\n",
        "        desc=f\"Grouping texts in chunks of {max_length}\",\n",
        "        )\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "2Aoim-JzyekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the preparation <a name=\"run\"></a>"
      ],
      "metadata": {
        "id": "5lxsJ9WbymA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(dataset_path: str,\n",
        "                 tokenizer,\n",
        "                 subset: int=8_000,\n",
        "                 max_length: int=512,\n",
        "                 return_labels: bool=False,\n",
        "                 ) -> Dataset:\n",
        "    \"\"\"Prepare the dataset for the compression.\"\"\"\n",
        "    dataset = load_from_disk(dataset_path)\n",
        "    dataset = tokenize_dataset(tokenizer, dataset, remove_columns=[\"text\"])\n",
        "    dataset = DatasetDict({corpus: group_texts(dataset=dataset[corpus],\n",
        "                                               max_length=max_length,\n",
        "                                               return_labels=return_labels)\n",
        "                                for corpus in dataset.keys()})\n",
        "    for corpus in dataset:\n",
        "        dataset[corpus] = dataset[corpus].remove_columns(\"corpus\").add_column(\"corpus\", [corpus] * len(dataset[corpus]))\n",
        "        if subset is None or len(dataset[corpus]) <= subset:\n",
        "            continue\n",
        "        dataset[corpus] = dataset[corpus].select(range(subset))\n",
        "    print(dataset)\n",
        "    dataset = concatenate_datasets(dataset.values())\n",
        "    n_sequences = len(dataset)\n",
        "    n_tokens = (len(dataset) * max_length)\n",
        "    print(f\"Total number of sequences: {n_sequences:,}. Total number of tokens: {n_tokens:,}\")\n",
        "    return dataset.shuffle()"
      ],
      "metadata": {
        "id": "lSoayIAdyrme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have defined all the required methods, we can the data preparation.\n",
        "\n",
        "In your opinion, why do we group the sequences?"
      ],
      "metadata": {
        "id": "zcfoCtGhkvyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"TODO\"\n",
        "tokenizer = \"TODO\""
      ],
      "metadata": {
        "id": "2udRzw3hjOOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1AdOBsqH14UJ8trxFQzizh85kr4TPBtL9",
      "authorship_tag": "ABX9TyOcN/1NQLviCzPcwsOXwN3v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/FeaturesMimicking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Introduction](https://www.google.com/)\n",
        "2. [Recall on Language Modeling and Transformers](https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/LLMs.ipynb)\n",
        "3. [Preparing the data](https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/Data.ipynb)\n",
        "4. [Low-Rank Approximation](https://colab.research.google.com/github/datacraft-paris/2311-Cerisara-LLM/blob/main/LowRankCompression.ipynb)\n",
        "5. Low-Rank Features Mimicking (This notebook)\n",
        "    1. [Weights are not Low-Rank... ðŸ˜¢](#not_low_rank)\n",
        "    2. [Features Mimicking](#ft_mm)\n",
        "    3. [Setup](#setup)\n",
        "    3. [Faster Low-Rank Linear](#fast)\n",
        "    4. [Forward Hooks](#hooks)\n",
        "        1. [Callable Hook Class](#callable)\n",
        "        2. [Register Hooks](#register)\n",
        "    5. [Train the LowRank Linears](#train)"
      ],
      "metadata": {
        "id": "zAuIU70Ze8kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weights are not Low-Rank... ðŸ˜¢ <a name=not_low_rank></a>"
      ],
      "metadata": {
        "id": "q3SxkI4sCg-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "So far, we have seen how to approximate the weights of the LLM with smaller weights. However, the compressed models is not too good. One reason is that the preatrained weights LLMs are often full rank. Studies have shown that transformer weights are often full rank compared to the activations (the outputs of the linear functions).\n",
        " <p align=\"center\">\n",
        "  <img src=\"https://github.com/datacraft-paris/2311-Cerisara-LLM/blob/main/illustration/weights_are_full.png?raw=true:, width=500\" alt=\"attention\" width=500 class=\"center\">\n",
        "</p>\n",
        "\n",
        "So, the authors in this paper proposed to decompose the weights using the activations."
      ],
      "metadata": {
        "id": "AG1K3sbX_jBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features Mimicking <a name=ft_mm><a/>"
      ],
      "metadata": {
        "id": "8MnHF-qBOeko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacraft-paris/2311-Cerisara-LLM/blob/main/illustration/Capture dâ€™Ã©cran 2024-02-25 Ã  01.56.15.png?raw=true:, width=500\" alt=\"attention\" width=500 class=\"center\">\n",
        "<br>\n",
        "    <em>\n",
        "    A figure that illustrate how we want to train low rank linear to output activations that mimick the activations of the pretrained weights.\n",
        "    <br>\n",
        "    The blue part represents the pretrained module and the pink the Low-Rank module trained to mimick the activations/\n",
        "    </em>\n",
        "</p>\n",
        "\n",
        "\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "Low-Rank approximation in our case can be formulated as this minimisation objective:\n",
        "\n",
        "$$\n",
        "\\underset{W_{1}, W_{2}}{\\mathrm{argmin}} \\;\\;\\|W - W_{1}W_{2}\\|_{F}^{2}\n",
        "$$\n",
        "\n",
        "Where $W_{1}$ and $W_{2}$ are Low-Rank matrices. An analytic solution of this problem is given by $SVD$, where $W_{1}$ and $W_{2}$ are defined as in the previous notebook (using $U$, $S$ and $V$)\n",
        "\n",
        "However, as we said, these matrices are probably not Low-Rank, hence the bad approximation. Instead, we want to approximate the weights using the linear activations, which seem Low-Rank. The linear activations are just defined as:\n",
        "\n",
        "$$\n",
        "f_{b}(X) = XW\n",
        "$$\n",
        "\n",
        "we want to find another linear function, with Low-Rank weights\n",
        "\n",
        "$$\n",
        "f_{a}(X) = XW_{1}W_{2}\n",
        "$$\n",
        "\n",
        "such that $f_{a}(X)$ ~ $f_{b}(X)$\n",
        "\n",
        "We can use a simple objective function to achieve this:\n",
        "\n",
        "$$\n",
        "\\underset{W_{1}, W_{2}}{\\mathrm{argmin}} \\;\\;\\|f_{b}(X) - f_{a}(X)\\|_{F}^{2}\n",
        "$$\n",
        "\n",
        "with this objective function, the Low-Rank weights will learn to mimick the activations returned by the full rank weight.\n",
        "\n",
        "We can find an analytic solution of this by using an _eighendecomposition_ of the covariance matrix of the activations.\n",
        "In our case, we will try to find a solution of this by using a gradient descent algorithme to minimize the Mean Squared Error."
      ],
      "metadata": {
        "id": "XfnfEkKHOiD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup <a name=setup></a>"
      ],
      "metadata": {
        "id": "y4gaKJ5QhZ4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import Dict, Set\n",
        "from argparse import ArgumentParser, BooleanOptionalAction\n",
        "from tqdm.auto import tqdm\n",
        "import yaml\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import PreTrainedModel, AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "baeelJFShbHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Load the model\n",
        "llm = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
        "                                           torch_dtype=torch.bfloat16)\n",
        "llm = llm.cuda()"
      ],
      "metadata": {
        "id": "WFIiIyWHkMiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: write a simple function that freeze all the parameters of the LLM\n",
        "def freeze_llm(llm):\n",
        "    \"\"\"A simple function that freeze the LLM by setting 'require_grad' to False.\"\"\"\n",
        "freeze_llm(llm)"
      ],
      "metadata": {
        "id": "cTRMZuBjkQP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster Low-Rank Linear <a name=fast></a>"
      ],
      "metadata": {
        "id": "YUX_9L9kIkr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same as previous low-rank linear. We just use the pytorch Linear to make it a bit faster, because built-in functions / classes are often better optimize."
      ],
      "metadata": {
        "id": "oNLw56u4SNzc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TlWppXFvUwy"
      },
      "outputs": [],
      "source": [
        "class LowRankLinear(torch.nn.Module):\n",
        "    def __init__(self, w, rank):\n",
        "        super().__init__()\n",
        "        self._decompose(w, rank)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _decompose(self, w, r):\n",
        "        u, s, v = torch.linalg.svd(w.to(dtype=torch.float32))\n",
        "        w1 = (u[:, :r]).to(dtype=torch.bfloat16)\n",
        "        w2 = (torch.diag(s)[:r, :r] @  v[:r, :]).to(dtype=torch.bfloat16)\n",
        "        linear_1 = torch.nn.Linear(in_features=w1.shape[0],\n",
        "                                   out_features=w.shape[-1],\n",
        "                                   bias=False,\n",
        "                                   dtype=torch.bfloat16,\n",
        "                                   device=w.device)\n",
        "        linear_1.weight = torch.nn.Parameter(w1)\n",
        "        linear_2 = torch.nn.Linear(in_features=w2.shape[0],\n",
        "                                   out_features=w2.shape[-1],\n",
        "                                   bias=False,\n",
        "                                   dtype=torch.bfloat16,\n",
        "                                   device=w.device)\n",
        "        linear_2.weight = torch.nn.Parameter(w2)\n",
        "        self.linear = torch.nn.Sequential(linear_2, linear_1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward Hooks <a name=hooks></a>"
      ],
      "metadata": {
        "id": "z0t-Dk5FRwht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A forward hook in pytorch is just a callable function that can be attached to any module. Each time the forward of the module is called, the hook attached to this module is called too. A forward hook in pytorch always has this signature:\n",
        "\n",
        "```python\n",
        "def forward_hook(module, input, output):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    - module:\n",
        "      The module to which the forward hook is attached.\n",
        "    - input:\n",
        "      The input to the module\n",
        "    - output:\n",
        "      The output produced by the model.\n",
        "    \"\"\"\n",
        "    stuff = do_stuff()\n",
        "    return stuff\n",
        "```\n",
        "\n",
        "We want to attach a low-rank linear to each linear module of the LLM. When the forward of linear module of the LLM is called, the corresponding low-rank linear is also called. This hook will just train the Low-Rank linear to mimick the output of the Linear module of the LLM, by minimizing `MSE` loss wia gradient descent."
      ],
      "metadata": {
        "id": "bR6hUgpVS3YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callable Hook <a name=callable></a>"
      ],
      "metadata": {
        "id": "9-NgEh7Ydujd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a callable class so we can store data."
      ],
      "metadata": {
        "id": "1vLjLk22TTBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ForwardHook:\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 lowrank_linear: torch.nn.Module,\n",
        "                 output_folder: Path,\n",
        "                 lr: float=0.000086,\n",
        "                 log_interval: int=8\n",
        "                 ):\n",
        "        self.name = name # The name of the linear module\n",
        "        self.weight_name = name.split(\".\")[-1]\n",
        "        self.layer_idx = re.search(r'\\d+', name).group()\n",
        "        self.lowrank_linear = lowrank_linear\n",
        "        self.optimizer = torch.optim.AdamW(self.lowrank_linear.parameters(), lr=lr)\n",
        "        self.losses = []\n",
        "        self.log_interval = log_interval\n",
        "        self.current_log_idx = 0\n",
        "        self.start = True\n",
        "        self.best_loss = float(\"Inf\")\n",
        "        self.output_folder = Path(output_folder)\n",
        "        self.output_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    def __call__(self, module, input, output) -> None:\n",
        "        \"\"\"Forward through the lowrank linear and collect loss.\"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "        student_output = self.lowrank_linear(input[0])\n",
        "        loss = \"TODO: use torch.functional to compute the mse loss between the student and the teacher\"\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.lowrank_linear.parameters(), max_norm=2.0, norm_type=2)\n",
        "        # TODO: Update the parameters of low rank linear here\n",
        "        self.losses.append(str(loss.item()))\n",
        "        if self.current_log_idx == self.log_interval or self.start:\n",
        "            self.start = False\n",
        "            self.current_log_idx = 0\n",
        "            print()\n",
        "            print(f\"Layer={self.layer_idx}, Module={self.weight_name}, Loss={loss.item()}\")\n",
        "            if self.best_loss > loss.item():\n",
        "                self.save()\n",
        "                self.best_loss = loss.item()\n",
        "        self.current_log_idx += 1\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"Save the low-rank module.\"\"\"\n",
        "        torch.save(self.lowrank_linear.state_dict(),\n",
        "                   f=self.output_folder / f\"{self.name}.pt\")\n",
        "\n",
        "    def save_losses(self):\n",
        "        with open(self.output_folder / f\"{self.name}.losses\", \"w\") as loss_file:\n",
        "            loss_file.write(\"\\n\".join(self.losses))\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.name)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.autoencoder.__str__()"
      ],
      "metadata": {
        "id": "eheEUkTqRxu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register Hooks <a name=register></a>"
      ],
      "metadata": {
        "id": "_D38hwJKSPcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def register_forward_hooks(config, model):\n",
        "    total = sum(1 for _ in model.named_modules())\n",
        "    hooks = set()\n",
        "    for name, module in tqdm(model.named_modules(), total=total):\n",
        "        module_name = name.split(\".\")[-1]\n",
        "        if module_name in config:\n",
        "            rank = config[module_name]\n",
        "            lowrank_linear = LowRankLinear(w=module.weight, rank=rank)\n",
        "            hook = ForwardHook(name=name,\n",
        "                               lowrank_linear=lowrank_linear,\n",
        "                               output_folder=\"lowrank_weights\")\n",
        "            module.register_forward_hook(hook)\n",
        "            hooks.add(hook)\n",
        "    return hooks"
      ],
      "metadata": {
        "id": "wVKU__SpSSTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"q_proj\": 384,\n",
        "    \"o_proj\": 384,\n",
        "    \"gate_proj\": 512,\n",
        "}"
      ],
      "metadata": {
        "id": "n4E7nWwriL62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Register the forward hooks\n",
        "#..."
      ],
      "metadata": {
        "id": "WM1uKwMdfyXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the LowRank Linears <a name=train></a>"
      ],
      "metadata": {
        "id": "W_z0bfeBaRfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: 1. Load the tokenized data\n",
        "train_data = \"TODO\""
      ],
      "metadata": {
        "id": "hMHJIYLVaVse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze_llm(llm)\n",
        "def forward_dataset(dataset: Dataset,\n",
        "                    llm: PreTrainedModel,\n",
        "                    batch_size: int=64\n",
        "                    ) -> None:\n",
        "    \"\"\"Forwards all the dataset through the LLM and computes the statistics.\"\"\"\n",
        "    dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "        inputs = batch[\"input_ids\"].to(llm.device)\n",
        "        llm(input_ids=inputs)"
      ],
      "metadata": {
        "id": "1vPqQG-3hBuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward_dataset(test, llm)"
      ],
      "metadata": {
        "id": "HthngyIsKJKE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
